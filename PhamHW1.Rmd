---
title: "Homework 1"
author: "Cuong Pham"
date: "1/23/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Question 1

a. I spent 8 hours on this homework

b. I think the homework length is reasonable

c. I worked independently but compared my answer with other in my cohort

### Question 2

We name the three boxes A, B, and C. Without the loss of generality, we assume that we choose box A and the the host opens box B. We are asked if we want to switch box A for box C. 

Let A =1 be the event that box A contains the prize, A = 2 be the event that box B has the prize, and A = 3 be the event that box C has the prize. 

Let B = 1 be the event that the host opens box B and B = 0 be the event that box B is not opened. 

Now, if we do not switch the the box, the probability of winning will be

$$P(A = 1 | B = 1) = \frac{P(B = 1 | A = 1)P(A = 1)}{P(B = 1)}$$
$$= \frac{(1/2)(1/3)}{P(B = 1| A = 1)P(A =1) + P(B = 1| A = 2)P(A =2) + P(B = 1|A =3)P(A=3)}$$
$$=\frac{(1/2)(1/3)}{(1/2)(1/3) + (0)(1/3) + (1)(1/3)} = \frac{1}{3}$$

Therefore, the probability of winning if we switch the box is 2/3. Hence, we should always switch the box.

### Question 3 

a. 


```{r}
q3 = function(theta1, y.theta1, y.theta0, y){
  theta0 = 1 - theta1
  if(y == 1){
    theta1.y = y.theta1*theta1/(y.theta1*theta1 + y.theta0*theta0)
    return(theta1.y)
  }else{
    y0.theta1 = 1 - y.theta1
    y0.theta0 = 1 - y.theta0
    
    theta0.y = y0.theta1*theta1/(y0.theta1*theta1 + y0.theta0*theta0)
    return(theta0.y)
  }
}
```

b. 

The value for $P(\theta =1 | y = 1)$

```{r}
theta1 = 0.001
y1.theta1 = 0.95
y1.theta0 = 0.05
  
q3(theta1 = 0.001, y.theta1 = 0.95, y.theta0 = 0.05, y = 1)
```

c. 

$P(y = 1| \theta = 1)$ is the probability that the test will show positive result given that the person has the disease. In here, this value is 0.95 means that the test gives the correct classification 95% on average when tested on the people that has the disease. $P(\theta =1 | y =1)$ is the probability that a person actually has the disease given that the person has a positive test result. This value is 0.0187 means that on average, among those tested positive, only 1.87% actually has the result. These number are not contradictory because they are looking at two different populations and measure two different things. $P(y = 1|\theta = 1)$ looks at the population of people that has the disease. Meanwhile, $P(\theta = 1| y = 1)$ looks at the population of people that are tested positive. If we have a rare disease, meaning the proportion of people with that disease is extremely small, we would not expected that proportion to be overwhemingly large in the group of people tested positive.    

d. 

```{r}
q3(theta1 = 0.001, y.theta1 = 0.8, y.theta0 = 0.25, y = 1)
```

e. 

$$P(y = 1| \theta = 1) = \frac{P(\theta=1| y = 1)*P(\theta =1)}{P(y = 1|\theta =1)*P(\theta =1 ) + P(y = 1|\theta =0)*P(\theta =0)}$$

$$= \frac{0.8*0.001}{0.8*0.001 + 0.25*(1-0.001)} = 0.003192975$$

f. The result in part a is larger than the result in part d. This makes sense because in part d, $P(y = 1 | \theta =1)$ is decreased to 0.85 and $P(Y = 1| \theta = 0)$ is increased to 0.25. This means that our test now is performing worse than before. Hence, $P(\theta =1 | y = 1)$ must also decrease. 


### Question 4

a, when $\alpha = \beta = 1$, our prior will be uninformative because now we have a uniform distribution.

```{r}
MM <- read.csv("C:/Users/cuong/Desktop/Baysian/mmdark2017.txt", sep="")
MM[1,]
sum(MM[1,])
```

We know that 

$$P(\theta|Y) \sim Beta(y + \alpha, n - y  + \beta ) = Beta(y + 1, n - y  + 1)$$

Hence, $P(\theta|Y_1) \sim Beta(2 + 1, 32 -2 +1) = Beta(3,31)$

b, 

```{r}
colSums(MM)

```


We have that the number of "success" is 2 and the number of "failure" is $32 -2 = 30$. According to Gelman et al., $\alpha -1$ equals to the number of success and $\beta -1$ equals to the number of failure. Hence, $\alpha = 3$ and $\beta = 31$

With our new $\beta$ and $\alpha$, $P(\theta|y) \sim Beta(92 + 3, 666 - 92 + 31) = Beta(95, 605)$

c. 

i, Normal approximation

$E[\theta|Y] = \frac{\alpha}{\alpha + \beta} = \frac{95}{95 + 605} = \frac{95}{700}= 0.1357$

$$SE = \sqrt{\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha + \beta +1 )}} = 0.0129$$

Hence, the 95% CI for $\theta$ is $0.1357 \pm 1.96*0.0129 = (0.110416,0.160984)$

ii, Random sample from beta posterior

The 95% CI 

```{r}
beta.sample = rbeta(5000, shape1 = 95, shape2 = 605)
quantile(beta.sample, c(0.025, 0.975))

```

d. 

```{r}

plot.beta <- function(alpha, bbeta, y, n) {
# arguments: alpha and bbeta are hyperparameters for beta prior,
# and y and n are observed successes and observed total from data
theta <- seq(0,.60,.001)
sampsize = alpha + bbeta - 2
# dbeta(x,shape1,shape2,ncp=0,log=F), where x=vector of quantiles,
# shape1=alpha, shape2=bbeta, ncp=noncentrality param, and if log=T it
# gives probabilities as log(p)
ttitle <- paste("Prior: alpha=",alpha,"bbeta=",bbeta)
plot(theta,dbeta(theta,y+alpha,n-y+bbeta),type="n",main=ttitle)
# Likelihood
lines(theta,dbeta(theta,y+1,n-y+1), lwd=2,col="red",lty=1)
# Prior
lines(theta,dbeta(theta,alpha,bbeta), lwd=2,col="blue",lty=3)
# Posterior
lines(theta,dbeta(theta,(y+alpha),(n-y+bbeta)),lwd=4,col="purple",lty=2)
}

plot.beta(1,1,90, 634)
plot.beta(3,31,90, 634)

```

In this graph, the red line is the likelihood, the blue line is prior and and purple line is posterior. Because the red line and the purple line are not very far apart. I would say the prior is not very informative.

